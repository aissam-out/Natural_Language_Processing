{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import an HTML web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response =  urllib.request.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
    "html = response.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BeautifulSoup to clean the webpage text of HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b_soup = BeautifulSoup(html,'html5lib')\n",
    "text = b_soup.get_text(strip = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we have clean text from the crawled web page, let’s convert the text into tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence = ['Good afternoon Mr. Doug, how are you?', 'I hope everything is going well.', 'I will call you soon, best regards.']\n",
      "word = ['Good', 'afternoon', 'Mr.', 'Doug', ',', 'how', 'are', 'you', '?', 'I', 'hope', 'everything', 'is', 'going', 'well', '.', 'I', 'will', 'call', 'you', 'soon', ',', 'best', 'regards', '.']\n"
     ]
    }
   ],
   "source": [
    "text_2 = \"Good afternoon Mr. Doug, how are you? I hope everything is going well. I will call you soon, best regards.\"\n",
    "print(\"sentence =\", nltk.tokenize.sent_tokenize(text_2))\n",
    "print(\"word =\", nltk.tokenize.word_tokenize(text_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fr = ['Bonjour M. Doug, comment allez-vous?', \"J'espère que tout va bien.\", 'Je vous contacterai prochainement, cordialement.']\n"
     ]
    }
   ],
   "source": [
    "text_fr = \"Bonjour M. Doug, comment allez-vous? J'espère que tout va bien. Je vous contacterai prochainement, cordialement.\"\n",
    "print(\"Fr =\", nltk.tokenize.sent_tokenize(text_fr,\"french\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Back to our main example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [t for t in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Words frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sr= nltk.corpus.stopwords.words('english')\n",
    "clean_tokens = tokens[:]\n",
    "for token in tokens:\n",
    "    if token in nltk.corpus.stopwords.words('english'):\n",
    "        clean_tokens.remove(token)\n",
    "\n",
    "freq = nltk.FreqDist(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "freq.plot(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text may contain stop words like ‘the’, ‘is’, ‘are’. Stop words can be filtered from the text to be processed. \n",
    "\n",
    "There is no universal list of stop words in nlp research, however the nltk module contains a list of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'dull', 'boy', '.', 'All', 'work', 'and', 'no', 'play', 'makes', 'jack', 'a', 'dull', 'boy', '.']\n"
     ]
    }
   ],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "words = nltk.tokenize.word_tokenize(data)\n",
    "wordsFiltered = []\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.', 'All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.']\n"
     ]
    }
   ],
   "source": [
    "for w in words:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)\n",
    " \n",
    "print(wordsFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'where', 'before', 'because', 'doesn', 'off', 'most', 'am', 'they', 'with', 've', 'theirs', 'y', 'above', 'isn', 'his', 'mightn', 'himself', \"weren't\", 'hers', 'it', \"couldn't\", 'do', 'over', 'aren', 'themselves', 'so', 'same', 'yourselves', 'few', 'about', 're', 'shan', 'again', \"she's\", 'weren', 'won', 'that', 'doing', 'has', 'from', \"mightn't\", 'as', \"that'll\", 'how', 'nor', \"hadn't\", 'once', \"mustn't\", 'been', 'ours', 'the', 'or', 'while', 'by', \"didn't\", 'between', 'down', 'my', 'have', \"you've\", 'these', 's', 'm', 'into', \"don't\", \"shan't\", \"you'd\", 'couldn', \"isn't\", 'whom', 'this', 'below', 'them', \"doesn't\", 'didn', 'our', 'wasn', 'wouldn', 't', 'its', 'can', 'those', 'be', 'a', 'don', \"hasn't\", 'other', \"it's\", 'did', 'very', 'when', 'will', 'through', 'during', 'under', 'needn', \"won't\", 'd', 'o', 'then', 'of', 'both', 'after', 'are', 'ain', 'should', 'at', 'he', 'yours', 'in', 'further', 'any', 'out', \"you'll\", 'shouldn', 'myself', 'was', 'for', 'their', \"aren't\", \"wouldn't\", \"shouldn't\", 'which', \"should've\", 'she', 'on', 'and', 'just', 'me', 'what', 'than', 'being', 'but', 'ourselves', 'itself', \"needn't\", 'own', 'not', 'each', 'does', 'haven', 'her', 'an', 'no', \"haven't\", 'some', 'herself', 'll', 'your', 'such', 'hadn', 'we', \"wasn't\", 'against', 'more', 'yourself', 'i', 'too', 'there', 'mustn', 'him', 'is', 'who', 'why', 'now', 'you', 'had', 'only', 'ma', 'having', 'up', 'here', 'were', 'hasn', 'until', 'if', 'to', \"you're\", 'all'}\n"
     ]
    }
   ],
   "source": [
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word stem is part of a word. It is sort of a normalization idea, but linguistic. <br>\n",
    "For example : A stemming algorithm reduces the words fishing, fished, and fisher to the stem fish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"process\",\"processing\",\"processed\",\"processes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process\n",
      "process\n",
      "process\n",
      "process\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "for word in words:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Whether', 'IN'), ('you', 'PRP'), (\"'re\", 'VBP'), ('new', 'JJ'), ('to', 'TO'), ('programming', 'VBG'), ('or', 'CC'), ('an', 'DT'), ('experienced', 'JJ'), ('developer', 'NN'), (',', ','), ('it', 'PRP'), (\"'s\", 'VBZ'), ('easy', 'JJ'), ('to', 'TO'), ('learn', 'VB'), ('and', 'CC'), ('use', 'VB'), ('Python', 'NNP'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "document = 'Whether you\\'re new to programming or an experienced developer, it\\'s easy to learn and use Python.'\n",
    "sentences = nltk.sent_tokenize(document)   \n",
    "for sent in sentences:\n",
    "    print(nltk.pos_tag(nltk.word_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the meanings of these speech codes : <br>\n",
    "<img src=\"images/codes.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, you can filter the data based on the type of word you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('new', 'JJ')\n",
      "('experienced', 'JJ')\n",
      "('easy', 'JJ')\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for sent in sentences:\n",
    "    data = data + nltk.pos_tag(nltk.word_tokenize(sent))\n",
    "    \n",
    "for word in data: \n",
    "    if 'JJ' in word[1]: \n",
    "        print(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
